"""
Simulation environments: linear tracks (with optional state uncertainty) and gridworlds.
"""
import numpy as np
from scipy.ndimage import gaussian_filter1d
import plotting as plt

__all__ = [
    'Track', 'DarkTrack', 'DynamicUncertaintyTrack', 'StaticUncertaintyTrack',
    'Wheel', 'KrauszGrid'
]

# ------------------------------------------------------------
# Track
# ------------------------------------------------------------
class Track:
    """1D Linear track environment for navigation tasks.
    
    Agent starts at position 0 and navigates toward a goal state.
    Reward is delivered at the goal.
    
    Attributes:
        n_states: Number of states in the track.
        goal_state: Index of the rewarded state.
        position: Current agent position.
        reward_function: Reward value at each state.
    """
    
    def __init__(self, n_states: int = 10, goal_state: int = 9,
                 reward_value: float = 1.0) -> None:
        self.n_states = n_states
        self.action_space = 2  # 0: left, 1: right
        self.reward_value = reward_value
        self.reward_function = np.zeros(n_states)
        self.set_goal_state(goal_state)
        self.position = 0
        self.step_size = 1

    def set_goal_state(self, goal_state: int) -> None:
        self.goal_state = goal_state
        self.reward_function[goal_state] = self.reward_value
        
    def reset(self) -> int:
        self.position = 0
        return self.position
    
    def step(self, action: int) -> tuple[int, float, bool]:
        end = self.position == self.goal_state
        reward = self.reward_function[self.position]
        if not end:
            if action == 0:  # left
                self.position = max(0, self.position - self.step_size)
            elif action == 1:  # right
                self.position = min(self.n_states-1, self.position + self.step_size) 
        return self.position, reward, end
    
    def teleport(self, destination: int) -> tuple[int, float, bool]:
        self.position = min(max(0, destination), self.n_states-1)
        reward = self.reward_function[self.position]
        end = self.position == self.goal_state
        return self.position, reward, end
    
    def pause(self, position: int) -> tuple[int, float, bool]:
        self.position = position
        reward = self.reward_function[self.position]
        end = self.position == self.goal_state
        return self.position, reward, end

class DarkTrack:
    """1D linear track with state uncertainty (Mikhael et al., 2022).
    
    Uncertainty generated by Gaussian-blurring of feature matrices (phi).
    Kernel width can vary across states to model increasing uncertainty.
    Porbability mass lost to Gaussian blurring is redistributed to nearest boundary state.
    
    Attributes:
        phi_t: Feature matrix with position-dependent uncertainty.
        n_states: Number of states in the track.
        goal_state: Index of the rewarded state.
        sigma_s: Kernel width at start of track.
        sigma_e: Kernel width at end of track.
        method: Method for computing kernel widths.
        plot_kernels: Whether to plot the kernel widths.
    """
    
    def __init__(self, n_states: int = 10, goal_state: int = 9,
                 reward_value: float = 1.0, start_width: float = 0.1,
                 end_width: float = 0.5, method: str = 'exponential',
                 sigma_s: float | None = None, sigma_e: float | None = None,
                 plot_kernels: bool = False) -> None:
        self.n_states = n_states
        self.goal_state = goal_state
        self.action_space = 2  # 0: left, 1: right
        self.step_size = 1
        self.reward_value = reward_value
        self.reward_function = np.zeros(n_states)
        
        self.sigma_s = sigma_s
        self.sigma_e = sigma_e
        self.method = method
        self.plot_kernels = plot_kernels
        self.set_phi()
        self.set_goal_state(goal_state)
        self.reset()

    def reset(self) -> int:
        self.position = 0
        return self.position
    
    def set_goal_state(self, goal_state: int) -> None:
        self.goal_state = goal_state
        self.reward_function[goal_state] = self.reward_value
    
    def _k_constant(self, width: float) -> np.ndarray:
        return np.full(self.n_states, width)
    
    def _k_exponential(self, sigma_s: float, sigma_e: float) -> np.ndarray:
        kernels = np.zeros(self.n_states)
        for i in range(self.n_states):
            i_norm = i / (self.n_states - 1) if self.n_states > 1 else 0  # Normalize to [0, 1]
            if sigma_s > 0:
                kernels[i] = sigma_s * ((sigma_e / sigma_s) ** i_norm)
            else:
                kernels[i] = sigma_s + (sigma_e - sigma_s) * (np.exp(i_norm) - 1) / (np.e - 1)
        return kernels
    
    def _compute_kernels(self, sigma_s: float, sigma_e: float,
                         method: str) -> np.ndarray:
        """Compute kernels based on method."""
        if method == 'constant':
            return self._k_constant(sigma_s)
        elif method == 'exponential':
            return self._k_exponential(sigma_s, sigma_e)
        else:
            raise ValueError(f"Unknown method: {method}. Must be 'constant' or 'exponential'")
    
    def set_phi(self) -> np.ndarray:
        kernels_t = self._compute_kernels(self.sigma_s, self.sigma_e, self.method)
        if self.plot_kernels:
            plt.plot_kernels(kernels_t, save_fig=True, filename="figs/darkening_track_kernels.pdf")
        
        self.phi_t = np.eye(self.n_states)
        for i in range(self.n_states):
            sigma = kernels_t[i]
            self.phi_t[i, :] = gaussian_filter1d(
                self.phi_t[i, :], sigma=sigma, mode='constant', cval=0.0
            )
            # Redistribute probability mass lost at boundaries
            row_sum = np.sum(self.phi_t[i, :])
            lost_pm = 1.0 - row_sum
            if lost_pm > 0:
                if i < self.n_states / 2:
                    self.phi_t[i, 0] += lost_pm
                else:
                    self.phi_t[i, self.goal_state] += lost_pm
        
        return self.phi_t

    def step(self, action: int) -> tuple[int, float, bool]:
        end = self.position == self.goal_state
        reward = self.reward_function[self.position]
        if not end:
            if action == 0:  # left
                self.position = max(0, self.position - self.step_size)
            elif action == 1:  # right
                self.position = min(self.n_states-1, self.position + self.step_size) 
        return self.position, reward, end

class DynamicUncertaintyTrack:
    """Track with different uncertainty for current vs. next state (Mikhael et al., 2022).
    
    phi_t uses narrow kernels; phi_t_prime uses broader kernels.
    Models increasing state uncertainty over time.
    
    Attributes:
        phi_t: Feature matrix for current state (narrow uncertainty).
        phi_t_prime: Feature matrix for next state (broad uncertainty).
        n_states: Number of states in the track.
        goal_state: Index of the rewarded state.
    """
    
    def __init__(self, n_states: int = 10, goal_state: int = 9,
                 reward_value: float = 1.0, s_width: float = 0.1,
                 l_width: float = 0.5) -> None:
        self.n_states = n_states
        self.goal_state = goal_state
        self.action_space = 2  # 0: left, 1: right
        self.step_size = 1
        self.reward_value = reward_value
        self.reward_function = np.zeros(n_states)
        self.s_width = s_width
        self.l_width = l_width
        self.set_phi()
        self.set_goal_state(goal_state)
        self.reset()

    def reset(self) -> int:
        self.position = 0
        return self.position
    
    def set_goal_state(self, goal_state: int) -> None:
        self.goal_state = goal_state
        self.reward_function[goal_state] = self.reward_value
    
    def set_phi(self) -> tuple[np.ndarray, np.ndarray]:
        self.phi_t = np.eye(self.n_states)
        for i in range(self.n_states):
            self.phi_t[i, :] = gaussian_filter1d(
                self.phi_t[i, :], sigma=self.s_width, mode='constant', cval=0.0
            )
            # Redistribute probability mass lost at boundaries
            row_sum = np.sum(self.phi_t[i, :])
            lost_pm = 1.0 - row_sum
            if lost_pm > 0:
                if i < self.n_states / 2:
                    self.phi_t[i, 0] += lost_pm
                else:
                    self.phi_t[i, self.goal_state] += lost_pm

        
        self.phi_t_prime = np.eye(self.n_states)
        for i in range(self.n_states):
            # Fix kernel width for final 5 states to 0.1
            kernel_width = 0.1 if i >= self.n_states - 5 else self.l_width
            self.phi_t_prime[i, :] = gaussian_filter1d(
                self.phi_t_prime[i, :], sigma=kernel_width, mode='constant', cval=0.0
            )
            row_sum = np.sum(self.phi_t_prime[i, :])
            lost_pm = 1.0 - row_sum
            if lost_pm > 0:
                if i < self.n_states / 2:
                    self.phi_t_prime[i, 0] += lost_pm
                else:
                    self.phi_t_prime[i, self.goal_state] += lost_pm

        return self.phi_t, self.phi_t_prime

    def step(self, action: int) -> tuple[int, float, bool]:
        end = self.position == self.goal_state
        reward = self.reward_function[self.position]
        if not end:
            if action == 0:  # left
                self.position = max(0, self.position - self.step_size)
            elif action == 1:  # right
                self.position = min(self.n_states-1, self.position + self.step_size) 
        return self.position, reward, end
    
class StaticUncertaintyTrack:
    """Track with constant state uncertainty (Mikhael et al., 2022).
    
    Both current and next state use the same blurred feature matrix.
    Serves as a control condition for DynamicUncertaintyTrack.
    
    Attributes:
        phi_t: Feature matrix for state representation.
        n_states: Number of states in the track.
        goal_state: Index of the rewarded state.
    """
    
    def __init__(self, n_states: int = 10, goal_state: int = 9,
                 reward_value: float = 1.0, s_width: float = 0.1) -> None:
        self.n_states = n_states
        self.goal_state = goal_state
        self.action_space = 2  # 0: left, 1: right
        self.step_size = 1
        self.reward_value = reward_value
        self.reward_function = np.zeros(n_states)
        self.s_width = s_width
        self.set_phi()
        self.set_goal_state(goal_state)
        self.reset()

    def reset(self) -> int:
        self.position = 0
        return self.position 
    
    def set_goal_state(self, goal_state: int) -> None:
        self.goal_state = goal_state
        self.reward_function[goal_state] = self.reward_value
    
    def set_phi(self) -> np.ndarray:
        self.phi_t = np.eye(self.n_states)
        for i in range(self.n_states):
            # Fix kernel width for final 5 states to 0.1
            kernel_width = 0.1 if i >= self.n_states - 5 else self.s_width
            self.phi_t[i, :] = gaussian_filter1d(
                self.phi_t[i, :], sigma=kernel_width, mode='constant', cval=0.0
            )
            # Redistribute probability mass lost at boundaries
            row_sum = np.sum(self.phi_t[i, :])
            lost_pm = 1.0 - row_sum
            if lost_pm > 0:
                if i < self.n_states / 2:
                    self.phi_t[i, 0] += lost_pm
                else:
                    self.phi_t[i, self.goal_state] += lost_pm

        return self.phi_t

    def step(self, action: int) -> tuple[int, float, bool]:
        end = self.position == self.goal_state
        reward = self.reward_function[self.position]
        if not end:
            if action == 0:  # left
                self.position = max(0, self.position - self.step_size)
            elif action == 1:  # right
                self.position = min(self.n_states-1, self.position + self.step_size) 
        return self.position, reward, end

# ------------------------------------------------------------
# Wheel
# ------------------------------------------------------------
class Wheel:
    """Running wheel environment (Guru et al., 2020).
    
    Similar to Track but with separate feature matrices for TD and MB systems.
    phi_TD encodes both temporal and spatial information.
    
    Attributes:
        phi_TD: Feature matrix for model-free learning.
        phi_MB: Feature matrix for model-based computation.
        n_states: Number of states.
        goal_state: Index of the rewarded state.
    """
    
    def __init__(self, n_states: int = 10, goal_state: int = 9,
                 reward_value: float = 1.0) -> None:
        self.n_states = n_states
        self.action_space = 2  # 0: back, 1: forward
        self.reward_value = reward_value
        self.set_phi()
        self.set_goal_state(goal_state)
        self.reset()

    def set_phi(self) -> None:
        self.phi_TD = np.eye(self.n_states) * 0.1  # temporal states
        self.phi_TD = np.hstack((self.phi_TD, np.ones((self.n_states, 1))))  # spatial state
        self.phi_MB = np.eye(self.n_states)
    
    def set_goal_state(self, goal_state: int) -> None:
        self.goal_state = goal_state
        self.reward_function = np.zeros(len(self.phi_TD[0]))
        self.reward_function[goal_state] = self.reward_value
        
    def reset(self) -> int:
        self.position = 0   
        return self.position
    
    def step(self, action: int) -> tuple[int, float, bool]:
        end = self.position == self.goal_state
        reward = self.reward_function[self.position]
        if action == 0:  # back
            self.position = max(0, self.position - 1)
        elif action == 1:  # forward
            self.position = min(self.n_states-1, self.position + 1)
        return self.position, reward, end

# ------------------------------------------------------------
# Grid World
# ------------------------------------------------------------
class KrauszGrid:
    """2D gridworld environment (Krausz et al., 2023).
    
    Agent navigates a grid toward a goal state.
    Probabilistic reward delivery at goal.
    
    Attributes:
        height: Number of rows in the grid.
        width: Number of columns in the grid.
        goal_state: (row, col) tuple of the goal position.
        p_reward: Probability of reward delivery at goal.
        reward_function: Reward values at each grid position.
    """
    
    def __init__(self, grid: tuple[int, int] = (8, 8),
                 goal: tuple[int, int] = (7, 7), p_reward: float = 1.0) -> None:
        self.height = grid[0]
        self.width = grid[1]
        self.goal_row = goal[0]
        self.goal_col = goal[1]
        self.p_reward = p_reward
        self.reward_function = np.zeros((self.height, self.width))
        self.set_reward_function(p_reward, goal)
        self.set_goal_state(goal)

    def reset(self, start_state: tuple[int, int] | None = None) -> tuple[int, int]:
        if start_state is None:
            self.position = np.random.randint(0, self.height), np.random.randint(0, self.width)
        else:
            self.position = start_state
        self.reward_value = np.random.binomial(1, self.p_reward)
        self.reward_function = np.zeros((self.height, self.width))
        self.reward_function[self.goal_state[0], self.goal_state[1]] = self.reward_value
        return self.position
    
    def set_reward_function(self, p_reward: float,
                            goal: tuple[int, int]) -> None:
        self.reward_value = np.random.binomial(1, p_reward)
        self.reward_function[goal[0], goal[1]] = self.reward_value
    
    def set_goal_state(self, goal_state: tuple[int, int] | None) -> None:
        if goal_state is None:
            goal_state = (np.random.randint(0, self.height), np.random.randint(0, self.width))
        self.goal_state = goal_state
    
    def step(self, action: int) -> tuple[tuple[int, int], float, bool]:
        end = self.position == self.goal_state
        reward = self.reward_function[self.position]
        if not end:
            if action == 0:  # up
                self.position = (max(0, self.position[0] - 1), self.position[1])
            elif action == 1:  # down
                self.position = (min(self.height-1, self.position[0] + 1), self.position[1])
            elif action == 2:  # left
                self.position = (self.position[0], max(0, self.position[1] - 1))
            elif action == 3:  # right
                self.position = (self.position[0], min(self.width-1, self.position[1] + 1))
        return self.position, reward, end
